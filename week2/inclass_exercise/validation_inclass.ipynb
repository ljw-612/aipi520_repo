{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to look at strategies to divide your dataset in order to perform model selection and testing using subsets of data in ways that do not create bias in your measurement of model performance.\n",
    "\n",
    "We are going to use a dataset which comes from a study done to try to use sonar signals to differentiate between a mine (simulated using a metal cylinder) and a rock.  Details on the dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we know we need\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(url):\n",
    "    # Load the data\n",
    "    data = pd.read_csv(url, header=None)\n",
    "    print(data.shape)\n",
    "    display(data.head())\n",
    "\n",
    "    # Separate into X and y \n",
    "    # Create feature matrix using the first 60 columns as the features\n",
    "    X = data.iloc[:,:60].to_numpy()\n",
    "    # Create target vector from the last column\n",
    "    y = data.iloc[:,60].to_numpy()\n",
    "\n",
    "    return X,y\n",
    "\n",
    "X,y = load_data('https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Training and test sets\n",
    "First, complete the `split()` function which does the following: \n",
    "- Splits your data into a feature matrix X and a target vector y  \n",
    "- THen splits the data into a training set and a test set, using `pct` percentage of the data for the test set.  Use `random_state=0` while splitting for repeatability.  Use the `stratify` parameter to ensure that the splits contain the same distribution of labels as the original data.\n",
    "\n",
    "Then, complete the function `run_model()` which does the following:\n",
    "- Trains (fit) your model on the training data \n",
    "- Uses your trained model to get predictions on the `X_test` test set and returns the predictions \n",
    "\n",
    "Finally, run the next code cell to calculate the display the accuracy of your classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(X,y,pct):\n",
    "    '''\n",
    "    Splits the data into training and test sets\n",
    "\n",
    "    Inputs:\n",
    "        X(np.ndarray): array of input data\n",
    "        y(np.ndarray): array of targets\n",
    "        pct(float): percentage of data to use for the test set\n",
    "\n",
    "    Returns:\n",
    "        X_train(np.ndarray): training set inputs\n",
    "        y_train(np.ndarray): training set targets\n",
    "        X_test(np.ndarray): test set inputs\n",
    "        y_test(np.ndarray): test set targets\n",
    "    '''\n",
    "    ### BEGIN SOLUTION ###\n",
    "    \n",
    "\n",
    "    ### END SOLUTION ###\n",
    "\n",
    "def run_model(X_train,y_train,X_test,model):\n",
    "    '''\n",
    "    Trains a model on the training data and then generates and returns predictions on the test set\n",
    "\n",
    "    Inputs:\n",
    "        X_train(np.ndarray): training set inputs\n",
    "        y_train(np.ndarray): training set targets\n",
    "        X_test(np.ndarray): test set inputs\n",
    "        model(sklearn.base.BaseEstimator): instantiated scikit-learn model object\n",
    "\n",
    "    Returns:\n",
    "        preds(np.ndarray): numpy array containing the model predictions for the test set\n",
    "    '''\n",
    "    ### BEGIN SOLUTION ###\n",
    "    \n",
    "\n",
    "    ### END SOLUTION ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the MLPClassifier algorithm and set the hyperparameter values\n",
    "model = MLPClassifier(hidden_layer_sizes=(100,50,10),activation='tanh',\n",
    "                      solver='sgd',learning_rate_init=0.001,max_iter=2000, random_state=0)\n",
    "                      \n",
    "# Evaluate the performance of our model using the test predictions\n",
    "X_train, y_train, X_test, y_test = split(X,y,pct=0.15)\n",
    "preds = run_model(X_train,y_train,X_test,model)\n",
    "assert len(preds) == len(y_test)\n",
    "acc_test = np.sum(preds==y_test)/len(y_test)\n",
    "print('Accuracy of our classifier on the test set is {:.3f}'.format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model selection using validation sets\n",
    "But what if we want to compare different models (for example, evaluate different algorithms or fine-tune our hyperparameters)?  Can we use the same strategy of training each model on the training data and then comparing their performance on the test set to select the best model?\n",
    "\n",
    "When we are seeking to optimize models by tuning hyperparameters or comparing different algorithms, it is a best practice to do so by comparing the performance of your model options using a \"validation\" set, and then reserve use of the test set to evaluate the performance of the final model you have selected.  To utilize this approach we must split our data three ways to create a training set, validation set, and test set.\n",
    "\n",
    "To illustrate this, let's compare two different models.  Complete the function below which performs the following:\n",
    "- Split your training set again into a training set and a validation set, using 15% of the training set for the new validation set (and the remaining 85% is still available for training). Use the `stratify` parameter to ensure that the splits contain the same distribution of labels as the original data.\n",
    "- Train (fit) the models provided as inputs on the training data\n",
    "- Now, use each of your trained models to generate predictions on the validation set inputs and calculate the accuracy of the predictions for each model.  Return the model with the higher validation set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(models,X_train,y_train):\n",
    "    '''\n",
    "    Compares models using a validation set and returns the model with the highest validation set performance\n",
    "\n",
    "    Inputs:\n",
    "        models(list): list of instantiated models to compare\n",
    "        X_train(np.ndarray): training set inputs\n",
    "        y_train(np.ndarray): training set targets\n",
    "\n",
    "    Returns:\n",
    "        best_model(sklearn.base.BaseEstimator): model with the highest validation set performance\n",
    "    '''\n",
    "    ### BEGIN SOLUTION ###\n",
    "    \n",
    "\n",
    "    ### END SOLUTION ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full,y_train_full,X_test,y_test = split(X,y,pct=0.15)\n",
    "\n",
    "# Create an instance of each model we want to evaluate\n",
    "model1 = MLPClassifier(hidden_layer_sizes=(100,50,10),activation='tanh',\n",
    "                      solver='sgd',learning_rate_init=0.001,max_iter=2000, random_state=0)\n",
    "\n",
    "model2 = MLPClassifier(hidden_layer_sizes=(100,50),activation='relu',\n",
    "                      solver='sgd',learning_rate_init=0.01,max_iter=2000, random_state=0)\n",
    "\n",
    "models = [model1,model2]\n",
    "best_model = compare_models(models,X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've chosen our final model, we can use the test set to evaluate it's performance.  Before we do that, let's retrain our model using the training plus validation data - since we are now done with model comparision we can use the validation set as part of our training data for our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our selected model on the full training data (training plus validation sets)\n",
    "best_model.fit(X_train_full,y_train_full)\n",
    "\n",
    "# Evaluate its performance on the test set\n",
    "preds_test = best_model.predict(X_test)\n",
    "acc_test = sum(preds_test==y_test)/len(y_test)\n",
    "print('Accuracy of our model on the test set is {:.3f}'.format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Model selection using cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common approach to comparing and optimizing models is to use cross-validation rather than a single validation set to compare model performace.  Complete the below function `run_kfolds()` which performs k-folds cross validation on the models provided as inputs.  Your function should use `nsplits` number of folds in the cross-validation and validation accuracy as the comparision metric.  After your model calculates the mean validation set accuracy for each model it should then return the model with the best performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfolds(models,X_train,y_train,nsplits):\n",
    "    '''\n",
    "    Performs k-folds cross validation on an arbitrary number of models provided as inputs and returns the model with the highest \n",
    "    validation set accuracy\n",
    "\n",
    "    Inputs:\n",
    "        X_train(np.ndarray): numpy array containing the training set features\n",
    "        y_train(np.ndarray): numpy array containing the training set labels\n",
    "        models(list): list of instantiated scikit-learn model objects to compare\n",
    "        nsplits(int): number of folds for cross-validation\n",
    "\n",
    "    Returns:\n",
    "        best_model(sklearn.base.BaseEstimator): model with the highest cross-validation accuracy\n",
    "    '''\n",
    "\n",
    "    ### BEGIN SOLUTION ###\n",
    "\n",
    "    \n",
    "        \n",
    "    ### END SOLUTION ###\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the two models we want to compare: a neural network model and a KNN model\n",
    "model2 = MLPClassifier(hidden_layer_sizes=(100,50),activation='relu',\n",
    "                    solver='sgd',learning_rate_init=0.01,max_iter=1000)\n",
    "model3 = LogisticRegression()\n",
    "models = [model2,model3]\n",
    "\n",
    "# Split data\n",
    "X_train, y_train, X_test, y_test = split(X,y,pct=0.15)\n",
    "\n",
    "# Run cross validation\n",
    "best_model = run_kfolds(models,X_train,y_train,nsplits=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the cross-validation accuracy of model2 is higher than model3, so we will use model2 as our best model.  Let's now evaluate its performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our selected model on the training plus validation sets\n",
    "preds = run_model(X_train,y_train,X_test,best_model)\n",
    "\n",
    "# Evaluate its performance on the test set\n",
    "acc_test = np.sum(preds==y_test)/len(y_test)\n",
    "print('Accuracy of our model on the test set is {:.3f}'.format(acc_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('aipi540')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31cc86d7aac4849c7546154c9b56d60163d5e8a1d83593a5eed18774fbf4fd37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
